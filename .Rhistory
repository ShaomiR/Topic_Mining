findFreqTerms(a.dtm1, N)
m <- as.matrix(a.dtm1)
v <- sort(rowSums(m), decreasing=TRUE)
head(v, N)
write.csv(findFreqTerms(a.dtm1, N), "keywords.csv")
corpus <- Corpus(VectorSource(tweets))
skipWords <- function(x) removeWords(x, stopwords("english"))
funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
a <- tm_map(corpus, FUN = tm_reduce, tmFuns = funcs)
a.dtm1 <- TermDocumentMatrix(a, control = list(wordLengths = c(3,10)))
N <- 30
findFreqTerms(a.dtm1, N)
m <- as.matrix(a.dtm1)
v <- sort(rowSums(m), decreasing=TRUE)
head(v, N)
write.csv(findFreqTerms(a.dtm1, N), "keywords.csv")
paragraph <- read.delim("paragraph.txt")
paragraph
write.csv(findFreqTerms(a.dtm1, N), "keywords.csv")
paragraph <- read.delim("paragraph.txt")
corpus_p = Corpus(VectorSource(paragraph))
skipWords <- function(x) removeWords(x, stopwords("english"))
funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
a <- tm_map(corpus_p, FUN = tm_reduce, tmFuns = funcs)
a.dtm2 <- TermDocumentMatrix(a, control = list(wordLengths = c(3,10)))
n <- 5
findFreqTerms(a.dtm2, n)
paragraph <- read.delim("paragraph.txt")
corpus_p = Corpus(VectorSource(paragraph))
skipWords <- function(x) removeWords(x, stopwords("english"))
funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
a <- tm_map(corpus_p, FUN = tm_reduce, tmFuns = funcs)
a.dtm2 <- TermDocumentMatrix(a, control = list(wordLengths = c(3,10)))
n <- 2
findFreqTerms(a.dtm2, n)
paragraph <- read.delim("paragraph.txt")
corpus_p = Corpus(VectorSource(paragraph))
skipWords <- function(x) removeWords(x, stopwords("english"))
funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
a <- tm_map(corpus_p, FUN = tm_reduce, tmFuns = funcs)
a.dtm2 <- TermDocumentMatrix(a, control = list(wordLengths = c(3,10)))
n <- 1
findFreqTerms(a.dtm2, n)
source('C:/Users/HP/Desktop/Topic_Mining/Topic_Mining.R')
paragraph <- read.delim("paragraph.txt")
corpus_p = Corpus(VectorSource(paragraph))
skipWords <- function(x) removeWords(x, stopwords("english"))
funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
a <- tm_map(corpus_p, FUN = tm_reduce, tmFuns = funcs)
a.dtm2 <- TermDocumentMatrix(a, control = list(wordLengths = c(3,10)))
n <- 1
findFreqTerms(a.dtm2, n)
paragraph <- read.delim("paragraph.txt")
paragraph
corpus2 = Corpus(VectorSource(paragraph))
corpus <- Corpus(VectorSource(tweets))
source('C:/Users/HP/Desktop/Topic_Mining/Topic_Mining.R')
corpus <- Corpus(VectorSource(tweets))
corpus2 = Corpus(VectorSource(paragraph))
corpus2
a2 = tm_map(corpus2 , FUN = tm_reduce)
a2 = tm_map(corpus2 , FUN = tm_reduce , tmFuns = funcs)
a2
a2.dtm1 = TermDocumentMatrix(a2,control = list(wordLengths = c(3,10)))
N2 = 2
findFreqTerms(a2.dtm1, N2)
findFreqTerms(a.dtm1, N)
paragraph <- read.delim("paragraph.txt")
paragraph
paragraph <- read.delim("paragraph.txt" , header = FALSE, sep = "", quote = "")
paragraph
corpus2 = Corpus(VectorSource(paragraph))
a2 = tm_map(corpus2 , FUN = tm_reduce , tmFuns = funcs)
a2.dtm1 = TermDocumentMatrix(a2,control = list(wordLengths = c(3,10)))
N2 = 2
findFreqTerms(a2.dtm1, N2)
paragraph <- read.delim("paragraph.txt" , header = FALSE, sep = "", quote = "")
paragraph
corpus2 = Corpus(VectorSource(paragraph))
a2 = tm_map(corpus2 , FUN = tm_reduce , tmFuns = funcs)
a2.dtm1 = TermDocumentMatrix(a2,control = list(wordLengths = c(3,10)))
N2 = 2
findFreqTerms(a2.dtm1, N2)
paragraph <- read.delim("paragraph.txt")
paragraph = tolower(paragraph)
paragraph
sort(table(paragraph), decreasing=T)[1:10]
keyforp = sort(table(paragraph), decreasing=T)[1:10]
write.csv(keyforp, "keyforp.csv")
s <- URISource(u)
corpus <- VCorpus(s)
paragraph <- read.delim("paragraph.txt")
myCorpus = Corpus(VectorSource(paragraph))
myStopWords = c(stopwords('english'))
paragraph <- read.delim("paragraph.txt")
myCorpus = Corpus(VectorSource(paragraph))
myStopWords = c(stopwords('english'))
myCorpus = tm_map(myCorpus,removeWords,myStopWords)
data("crude")
paragraph <- read.delim("paragraph.txt")
myCorpus = Corpus(VectorSource(paragraph))
myStopWords = c(stopwords('english'))
myCorpus = tm_map(myCorpus,removeWords,myStopWords)
data("crude")
myTdm = as.matrix(TermDocumentMatrix(crude))
FreqMat <- data.frame(ST = rownames(myTdm),
Freq = rowSums(myTdm),
row.names = NULL)
head(FreqMat, 2)
paragraph
paragraph <- read.delim("paragraph.txt")
myCorpus = Corpus(VectorSource(paragraph))
myStopWords = c(stopwords('english'))
myCorpus = tm_map(myCorpus,removeWords,myStopWords)
data("crude")
myTdm = as.matrix(TermDocumentMatrix(crude))
FreqMat <- data.frame(ST = rownames(myTdm),
Freq = rowSums(myTdm),
row.names = NULL)
head(FreqMat, 2)
paragraph <- read.delim("paragraph.txt")
myCorpus = Corpus(VectorSource(paragraph))
paragraph <- read.delim("paragraph.txt")
myCorpus = Corpus(VectorSource(paragraph))
a <- tm_map(myCorpus, FUN = tm_reduce, tmFuns = funcs)
a.dtm1 <- TermDocumentMatrix(a, control = list(wordLengths = c(3,10)))
N <- 2
findFreqTerms(a.dtm1, N)
a
paragraph <- read.delim("paragraph.txt")
myCorpus = Corpus(VectorSource(paragraph))
a2 <- tm_map(myCorpus, FUN = tm_reduce, tmFuns = funcs)
a2.dtm1 <- TermDocumentMatrix(a2, control = list(wordLengths = c(3,10)))
N <- 2
findFreqTerms(a.dtm1, N)
paragraph <- read.delim("paragraph.txt")
myCorpus = Corpus(VectorSource(paragraph))
a2 <- tm_map(myCorpus, FUN = tm_reduce, tmFuns = funcs)
a2.dtm1 <- TermDocumentMatrix(a2, control = list(wordLengths = c(3,10)))
N <- 2
findFreqTerms(a2.dtm1, N)
paragraph <- read.delim("paragraph.txt")
myCorpus = Corpus(VectorSource(paragraph))
a2 <- tm_map(myCorpus, FUN = tm_reduce)
a2.dtm1 <- TermDocumentMatrix(a2, control = list(wordLengths = c(3,10)))
N <- 2
findFreqTerms(a2.dtm1, N)
paragraph <- read.delim("paragraph.txt")
myCorpus = Corpus(VectorSource(paragraph))
a2 <- tm_map(myCorpus, FUN = tm_reduce, tmFuns = funcs)
a2.dtm1 <- TermDocumentMatrix(a2, control = list(wordLengths = c(3,10)))
N2 <- 1
findFreqTerms(a2.dtm1, N2)
write.csv(findFreqTerms(a2.dtm1, N2), "keyword.csv")
a2.dtm1
findFreqTerms(a.dtm1, N)
some_tweets = read.csv("rawTweets.csv")
tweets <-some_tweets$x
#Figure the most frequent words
corpus <- Corpus(VectorSource(tweets))
skipWords <- function(x) removeWords(x, stopwords("english"))
funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
a <- tm_map(corpus, FUN = tm_reduce, tmFuns = funcs)
a.dtm1 <- TermDocumentMatrix(a, control = list(wordLengths = c(3,10)))
N <- 30
findFreqTerms(a.dtm1, N)
m <- as.matrix(a.dtm1)
v <- sort(rowSums(m), decreasing=TRUE)
head(v, N)
write.csv(findFreqTerms(a.dtm1, N), "keywords.csv")
paragraph <- read.delim("paragraph.txt")
myCorpus = Corpus(VectorSource(paragraph))
a2 <- tm_map(myCorpus, FUN = tm_reduce, tmFuns = funcs)
a2.dtm1 <- TermDocumentMatrix(a2, control = list(wordLengths = c(3,10)))
N2 <- 1
findFreqTerms(a2.dtm1, N2)
write.csv(findFreqTerms(a2.dtm1, N2), "keyword.csv")
findFreqTerms(a.dtm1, N)
source('C:/Users/HP/Desktop/Topic_Mining/Topic_Mining.R')
setwd("C:/Users/HP/Desktop/Topic_Mining")
# Install
# Load
library("tm")
library("SnowballC")
library("wordcloud")
#get the tweets
some_tweets = read.csv("rawTweets.csv")
tweets <-some_tweets$x
#Figure the most frequent words
corpus <- Corpus(VectorSource(tweets))
skipWords <- function(x) removeWords(x, stopwords("english"))
funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
a <- tm_map(corpus, FUN = tm_reduce, tmFuns = funcs)
a.dtm1 <- TermDocumentMatrix(a, control = list(wordLengths = c(3,10)))
N <- 30
findFreqTerms(a.dtm1, N)
m <- as.matrix(a.dtm1)
v <- sort(rowSums(m), decreasing=TRUE)
head(v, N)
write.csv(findFreqTerms(a.dtm1, N), "keywords.csv")
paragraph <- read.delim("paragraph.txt")
myCorpus = Corpus(VectorSource(paragraph))
a2 <- tm_map(myCorpus, FUN = tm_reduce, tmFuns = funcs)
a2.dtm1 <- TermDocumentMatrix(a2, control = list(wordLengths = c(3,10)))
N2 <- 1
findFreqTerms(a2.dtm1, N2)
write.csv(findFreqTerms(a2.dtm1, N2), "keyword.csv")
paragraph <- read.delim("paragraph2.txt")
myCorpus = Corpus(VectorSource(paragraph))
a2 <- tm_map(myCorpus, FUN = tm_reduce, tmFuns = funcs)
a2.dtm1 <- TermDocumentMatrix(a2, control = list(wordLengths = c(3,10)))
N2 <- 1
findFreqTerms(a2.dtm1, N2)
write.csv(findFreqTerms(a2.dtm1, N2), "keyword.csv")
paragraph <- read.delim("paragraph2.txt")
myCorpus = Corpus(VectorSource(paragraph))
a2 <- tm_map(myCorpus, FUN = tm_reduce, tmFuns = funcs)
a2.dtm2 <- TermDocumentMatrix(a2, control = list(wordLengths = c(3,10)))
N2 <- 1
findFreqTerms(a2.dtm2, N2)
write.csv(findFreqTerms(a2.dtm2, N2), "keyword.csv")
filePath <- "paragraph.txt"
text <- readLines(filePath)
filePath <- "paragraph2.txt"
text <- readLines(filePath)
text
filePath <- "paragraph2.txt"
text <- readLines(filePath)
docs <- Corpus(VectorSource(text))
inspect(docs)
filePath <- "paragraph2.txt"
text <- readLines(filePath)
docs <- Corpus(VectorSource(text))
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs
filePath <- "paragraph2.txt"
text <- readLines(filePath)
docs <- Corpus(VectorSource(text))
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
docs
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 2)
source('C:/Users/HP/Desktop/Topic_Mining/Topic_Mining.R')
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 2)
write.csv(head(d,2),"keywordfromP.csv")
source('C:/Users/HP/Desktop/Topic_Mining/Topic_Mining.R')
keywordfromT = read.csv("keywords.csv")
keywordfromT = keywordfromT$x
keywordfromT
keywordfromT = read.csv("keywords.csv")
keywordfromT = keywordfromT$x
keywordfromP = read.csv("keywordfromT.csv")
keywordfromP = keywordfromP$word
keywordfromP
keywordfromP = read.csv("keywordfromP.csv")
keywordfromP = keywordfromP$word
keywordfromP
source('C:/Users/HP/Desktop/Topic_Mining/Topic_Mining.R')
keywordfromP
for(word in keywordfromT){
print(paste(word))
}
match = 0
matchcount = 0
for(wordT in keywordfromT){
for(wordP in keywordfromP){
matchcount = matchcount +1
}
}
matchcount
for(wordT in keywordfromT){
for(wordP in keywordfromP){
if(identical(wordP , wordT))
matchcount = matchcount +1
}
}
matchcount
for(wordT in keywordfromT){
for(wordP in keywordfromP){
if(identical(wordP , wordT))
print(wordP , wordT)
matchcount = matchcount +1
}
}
for(wordT in keywordfromT){
for(wordP in keywordfromP){
if(identical(wordP , wordT))
print(paste(wordP))
print(paste(wordT))
matchcount = matchcount +1
}
}
for (wordP in keywordfromP) {
print(paste(wordP))
}
match = 0
matchcount = 0
for (wordP in keywordfromP) {
print(paste(wordP))
for(wordT in keywordfromT){
if (wordT == wordP){
matchcount = matchcount +1
}
}
}
matchcount
if(matchcount >= (keywordfromT * 0.50) )
{
print(paste("The topics are related"))
}
if(matchcount >= (keywordfromT*0.50) )
{
print(paste("The topics are related"))
}
keywordfromT.length
length.keywordT
length.keywordfromT
length(keywordfromT)
length(keywordfromT)*.50
length(keywordfromP)*.50
if(matchcount >= length(keywordfromP)*.50 )
{
print(paste("The topics are related"))
}
if(matchcount >= ceiling(length(keywordfromP)*.50) )
{
print(paste("The topics are related"))
}
else {
print(paste("The topics are not related"))
}
source('C:/Users/HP/Desktop/Topic_Mining/Topic_Mining.R')
for (wordP in keywordfromP) {
print(paste("The words that we are going to check are "+wordP))
for(wordT in keywordfromT){
if (wordT == wordP){
matchcount = matchcount +1
}
}
}
for (wordP in keywordfromP) {
print(paste("The words that we are going to check are: "))
print(paste(wordP))
for(wordT in keywordfromT){
if (wordT == wordP){
matchcount = matchcount +1
}
}
}
source('C:/Users/HP/Desktop/Topic_Mining/Topic_Mining.R')
setwd("C:/Users/HP/Desktop/Topic_Mining")
# Install
# Load
library("tm")
library("SnowballC")
library("wordcloud")
#get the tweets
some_tweets = read.csv("rawTweets.csv")
tweets <-some_tweets$x
#Figure the most frequent words
corpus <- Corpus(VectorSource(tweets))
skipWords <- function(x) removeWords(x, stopwords("english"))
funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
a <- tm_map(corpus, FUN = tm_reduce, tmFuns = funcs)
a.dtm1 <- TermDocumentMatrix(a, control = list(wordLengths = c(3,10)))
N <- 30
findFreqTerms(a.dtm1, N)
m <- as.matrix(a.dtm1)
v <- sort(rowSums(m), decreasing=TRUE)
head(v, N)
write.csv(findFreqTerms(a.dtm1, N), "keywords.csv")
filePath <- "paragraph2.txt"
text <- readLines(filePath)
docs <- Corpus(VectorSource(text))
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 2)
write.csv(head(d,2),"keywordfromP.csv")
keywordfromT = read.csv("keywords.csv")
keywordfromT = keywordfromT$x
keywordfromP = read.csv("keywordfromP.csv")
keywordfromP = keywordfromP$word
matchcount = 0
for (wordP in keywordfromP) {
for(wordT in keywordfromT){
if (wordT == wordP){
matchcount = matchcount +1
}
}
}
if(matchcount >= ceiling(length(keywordfromP)*.50) )
{
print(paste("The topics are related"))
}
setwd("C:/Users/HP/Desktop/Topic_Mining")
# Install
# Load
library("tm")
library("SnowballC")
library("wordcloud")
#get the tweets
some_tweets = read.csv("rawTweets.csv")
tweets <-some_tweets$x
#Figure the most frequent words
corpus <- Corpus(VectorSource(tweets))
skipWords <- function(x) removeWords(x, stopwords("english"))
funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
a <- tm_map(corpus, FUN = tm_reduce, tmFuns = funcs)
a.dtm1 <- TermDocumentMatrix(a, control = list(wordLengths = c(3,10)))
N <- 30
findFreqTerms(a.dtm1, N)
m <- as.matrix(a.dtm1)
v <- sort(rowSums(m), decreasing=TRUE)
head(v, N)
write.csv(findFreqTerms(a.dtm1, N), "keywords.csv")
filePath <- "paragraph2.txt"
text <- readLines(filePath)
docs <- Corpus(VectorSource(text))
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 2)
write.csv(head(d,2),"keywordfromP.csv")
keywordfromT = read.csv("keywords.csv")
keywordfromT = keywordfromT$x
keywordfromP = read.csv("keywordfromP.csv")
keywordfromP = keywordfromP$word
matchcount = 0
for (wordP in keywordfromP) {
for(wordT in keywordfromT){
if (wordT == wordP){
matchcount = matchcount +1
}
}
}
if(matchcount >= ceiling(length(keywordfromP)*.50) )
{
print(paste("The topics are related"))
}
